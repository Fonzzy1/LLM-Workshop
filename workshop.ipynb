{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/Fonzzy1/LLM-Workshop/blob/main/workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# LLMs for Communications Methods\n",
        "\n",
        "## Before you proceed\n",
        "\n",
        "Since by default the runtime type of Colab instance is CPU based, in order to use LLM models make sure to change your runtime type to T4 GPU (or better if you're a paid Colab user). This can be done by going to **Runtime > Change runtime type**.\n",
        "\n",
        "While running your script be mindful of the resources you're using. This can be tracked at **Runtime > View resources**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install ollama kagglehub kagglehub[pandas-datasets]\n",
        "# Fancy little subprocess trick to get ollama working in colab books\n",
        "import subprocess\n",
        "proccess = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL,\n",
        "stderr=subprocess.DEVNULL, stdin=subprocess.DEVNULL, close_fds=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pulling Models\n",
        "\n",
        "We are going to be using a few models today: Lamma, Qwen, Gemma and Deepseek.  \n",
        "All of these models are open source, and all but Gemma are the same size.  \n",
        "However, they will exhibit slight differences when we ask them the same\n",
        "question.\n",
        "\n",
        "For other models check https://ollama.com/library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!ollama pull llama3.1:8b\n",
        "!ollama pull qwen3:8b\n",
        "!ollama pull gemma3:4b\n",
        "!ollama pull deepseek-r1:8b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interacting with LLMS in python\n",
        "\n",
        "Unlike interacting with a chat client online, using LLMs in python is much more\n",
        "flexible but takes a little more time to set up.\n",
        "\n",
        "In python we can make functions - repeatable peices of code using the `def`\n",
        "syntax. We can give these fucntions arguments which we then use to change how it\n",
        "behaves. \n",
        "\n",
        "Below, there is a function that takes a prompt and the name of the model we want\n",
        "to use and then gives back the response from the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import ollama\n",
        "\n",
        "def query_llm(prompt, model):\n",
        "    \"\"\"\n",
        "    Given a 'prompt' and the name of a model,\n",
        "    return the LLM's text response (uses ollama SDK).\n",
        "    Because the model has a default, we don't need to be explicit in which model\n",
        "    to use if you don't want to.\n",
        "    \"\"\"\n",
        "    # Send the request to Ollama and get the response dictionary (a kind of\n",
        "    # \"named list\").\n",
        "    response = ollama.chat(\n",
        "        model=model, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    # Return ONLY the LLM's textual answer from the response.\n",
        "    return response.message.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a function, we can look at some responses from the different\n",
        "models.\n",
        "\n",
        "Change the question in the cell below to something that is within your field of\n",
        "expertise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "question = 'What is the role of performance in australian parliamentary debates'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now go and ask what each model says in response using the function that\n",
        "was defined above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for model in [ 'llama3.1:8b', 'qwen3:8b', 'gemma3:4b', 'deepseek-r1:8b']:\n",
        "    answer = query_llm(question, model)\n",
        "    print(f'{model} says:\\n{answer}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What did the model get right, what did the model get wrong? How did the response\n",
        "differ between the different models. If you want, try playing with a couple of\n",
        "different questions or different ways of wording the questions to see if you get\n",
        "different results.\n",
        "\n",
        "## How language models are used in communications research\n",
        "\n",
        "While we can use language models for question-answering tasks, as we did before,\n",
        "they are much more useful for the busy work of research. If we think of language\n",
        "models as cheap and fast RAs, we can start offloading some of the annoying tasks\n",
        "-- labelling, data cleaning, entity extraction, etc. -- to them and open up our\n",
        "work to much larger datasets and bigger questions than we would have been able\n",
        "to approach before.\n",
        "\n",
        "\n",
        "### Narrative Framing -- Hero Villan Extraction\n",
        "\n",
        "In this demo, we're going to expand on the work done by [Frermann et al.\n",
        "(2023)](https://doi.org/10.18653/v1/2023.acl-long.486), which looks at how\n",
        "narrative actors -- Heroes, Victims and Villains -- are allocated within climate\n",
        "discourse.\n",
        "\n",
        "#### Dataset Selection\n",
        "\n",
        "Dataset selection is one of the most important parts of computational\n",
        "communications tasks as it defines the scope of questions that can be answered\n",
        "by your later analysis.\n",
        "\n",
        "We are using a pre-built dataset from\n",
        "[Kaggle](https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset), which we do because it is easy to access.\n",
        "\n",
        "In this dataset, we have tweets, with their sentiment towards climate change\n",
        "labeled as so:\n",
        "- 2 (News): the tweet links to factual news about climate change  \n",
        "- 1 (Pro): the tweet supports the belief of man-made climate change  \n",
        "- 0 (Neutral): the tweet neither supports nor refutes the belief of man-made\n",
        "  climate change  \n",
        "- -1 (Anti): the tweet does not believe in man-made climate change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"twitter_sentiment_data.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"edqian/twitter-climate-change-sentiment-dataset\",\n",
        "  file_path,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can see the first few records and how the sentiment is distribuited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"First 5 records:\\n\", df.head())\n",
        "\n",
        "print(\"Sentiment Distribuiton\")\n",
        "print(df['sentiment'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing and Cleaning\n",
        "\n",
        "@Laura Have a go here\n",
        "\n",
        "\n",
        "#### Building the infrustructure\n",
        "\n",
        "While we have the data and the models, we need the code to make them interact\n",
        "with each other. So, before we start writing prompts, we need to have a look at\n",
        "how we can make the LLM interact with the data in a clean and reproducible way.\n",
        "\n",
        "The first thing to do is build a response format. Think of this as the form that\n",
        "the LLM will fill out when we ask it to look at tweet. In our example, we are\n",
        "asking the LLM to identify the hero, the villan and the victim in the tweet, so\n",
        "our response format will look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "\n",
        "class ResponseFormat(BaseModel):\n",
        "    hero: Optional[str]\n",
        "    victim: Optional[str]\n",
        "    villain: Optional[str]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second thing we need is a new function that can use this response format and\n",
        "the feed the data to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def transform_data(df, prompt, model, response_format):\n",
        "    cols = response_format.__fields__.keys()\n",
        "    for col in cols:\n",
        "        df[col] = None\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        response = ollama.chat(\n",
        "            model=model,\n",
        "            messages=[{'role': 'system', 'content': prompt}, {'role': 'user',\n",
        "            'content': row['message']}],\n",
        "            format=response_format.model_json_schema(),\n",
        "        )\n",
        "        parsed_response =\n",
        "        response_format.model_validate_json(response.message.content)\n",
        "        for col in cols:\n",
        "            df.at[idx, col] = getattr(parsed_response, col)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
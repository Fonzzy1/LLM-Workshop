{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fonzzy1/LLM-Workshop/blob/main/workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs for Communications Methods\n",
        "\n",
        "## Before you proceed\n",
        "---\n",
        "\n",
        "Since by default the runtime type of Colab instance is CPU based, in order to use LLM models make sure to change your runtime type to T4 GPU (or better if you're a paid Colab user). This can be done by going to **Runtime > Change runtime type**.\n",
        "\n",
        "While running your script be mindful of the resources you're using. This can be tracked at **Runtime > View resources**.\n"
      ],
      "metadata": {
        "id": "zyGk-87qnbWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "---\n",
        "\n",
        "These are needed to make colab work with Ollama -- a local LLM inferance platform.\n",
        "Ollama needs to be running in the background, so the second block will set this up.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B1S1YL6EnYBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "YlVK9iG4AD5L",
        "collapsed": true,
        "outputId": "bb27868c-8567-4509-9e02-3f49066a49fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,204 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,543 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,964 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,410 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,850 kB]\n",
            "Fetched 38.1 MB in 8s (4,597 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 2s (200 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "Jh5CBAFxBYAC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pulling Model\n",
        "---\n",
        "\n",
        "Download the LLM model using `ollama pull llama3.2`.\n",
        "\n",
        "For other models check https://ollama.com/library"
      ],
      "metadata": {
        "id": "WcBLqZfyoHg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "o2ghppmRDFny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LsQHyDkJi-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And that's it!\n",
        "---\n",
        "\n",
        "With this you should be able to freely play around with the models in your scripts. Following is an example using `langchain-ollama` to answer a simple prompt.\n",
        "\n",
        "If you have a use-case that can help out others feel free to add your notebook to [Collama](https://github.com/5aharsh/collama/fork)"
      ],
      "metadata": {
        "id": "TYQJNeTuni_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama PyPDF2"
      ],
      "metadata": {
        "id": "MbrT39oil6tK",
        "outputId": "8542c660-81d6-4943-fe51-0084106db84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "def query_llm(prompt, model=\"llama3.2\"):\n",
        "    \"\"\"\n",
        "    Given a 'prompt' and the name of a model,\n",
        "    return the LLM's text response (uses ollama SDK).\n",
        "    Because the model has a default, we don't need to be explicit in which model\n",
        "    to use if you don't want to.\n",
        "    \"\"\"\n",
        "    # Send the request to Ollama and get the response dictionary (a kind of\n",
        "    # \"named list\").\n",
        "    response = ollama.chat(\n",
        "        model=model, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    # Return ONLY the LLM's textual answer from the response.\n",
        "    return response.message.content\n",
        "\n",
        "\n",
        "# --- Using (calling) a function ---\n",
        "# To use a function, write its name and provide any needed input (\"arguments\")\n",
        "# in parentheses.\n",
        "\n",
        "# Let's set up a question prompt for the LLM:\n",
        "prompt = (\n",
        "    \"What is the state of the art method for computational framining analysis?\"\n",
        ")\n",
        "\n",
        "# Now, call our function with this prompt to get the LLM's answer:\n",
        "llm_answer = query_llm(prompt)\n",
        "\n",
        "# Finally, print out the LLM's answer for everyone to see!\n",
        "print(\"Ollama LLM says:\\n\")\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "id": "9quBP56zDvpt",
        "outputId": "8b52ed2c-b3f6-4cda-8ee8-c786a4b898d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama LLM says:\n",
            "\n",
            "The state-of-the-art methods for computational framing analysis, which is also known as sentiment analysis or opinion mining, have evolved significantly over the years. Here are some of the current approaches:\n",
            "\n",
            "1. **Deep Learning Models**: Deep learning models, particularly Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), have become widely used for sentiment analysis. These models can learn complex patterns in text data and achieve high accuracy.\n",
            "2. **BERT (Bidirectional Encoder Representations from Transformers)**: BERT is a pre-trained language model that has achieved state-of-the-art results in many natural language processing tasks, including sentiment analysis. It uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in the input text.\n",
            "3. **Transformers**: Transformers are another type of deep learning model that have gained popularity in recent years. They use self-attention mechanisms to model complex relationships between words in the input text and can be fine-tuned for specific tasks like sentiment analysis.\n",
            "4. **Graph Convolutional Networks (GCNs)**: GCNs are a type of neural network that are designed to model graph-structured data, such as social networks or word co-occurrence networks. They have been used to analyze the structural relationships between words in text data and improve sentiment analysis performance.\n",
            "5. **Attention-Based Models**: Attention-based models, which use attention mechanisms to focus on specific parts of the input text, have also shown promising results in sentiment analysis.\n",
            "\n",
            "Some popular techniques used in state-of-the-art methods for computational framing analysis include:\n",
            "\n",
            "1. **Word Embeddings**: Word embeddings, such as Word2Vec and GloVe, are used to represent words as dense vectors that capture their semantic meaning.\n",
            "2. **Aspect-Based Sentiment Analysis (ABSA)**: ABSA is a sub-task of sentiment analysis that focuses on identifying specific aspects or entities in the input text that are related to the sentiment.\n",
            "3. **Multi-Task Learning**: Multi-task learning, which involves training models on multiple tasks simultaneously, has been shown to improve performance in sentiment analysis by capturing more nuanced relationships between words and contexts.\n",
            "\n",
            "Some popular frameworks and tools for implementing these methods include:\n",
            "\n",
            "1. **TensorFlow**: TensorFlow is an open-source machine learning framework developed by Google that provides a wide range of tools and libraries for building deep learning models.\n",
            "2. **PyTorch**: PyTorch is another popular open-source machine learning framework that provides a dynamic computation graph and automatic differentiation capabilities.\n",
            "3. **Transformers with PyTorch or TensorFlow**: There are also pre-trained transformer models available in PyTorch and TensorFlow, which can be fine-tuned for specific tasks like sentiment analysis.\n",
            "\n",
            "Overall, the state-of-the-art methods for computational framing analysis rely on deep learning models that can learn complex patterns in text data and capture nuanced relationships between words and contexts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyPDF2 for PDF reading\n",
        "import PyPDF2\n",
        "\n",
        "\n",
        "# Using the PyPDF library, we can write a function that will read a Pdf and\n",
        "# extract out all the text\n",
        "def read_pdf_text(pdf_path):\n",
        "    \"\"\"\n",
        "    Opens a PDF file from the provided path and returns all its text as one string.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "# ---- Main workflow ----\n",
        "\n",
        "pdf_path = \"example.pdf\"  # Change this to your PDF file\n",
        "\n",
        "# Use our function to read the PDF\n",
        "pdf_text = read_pdf_text(pdf_path)\n",
        "\n",
        "# Create a prompt that includes the PDF's contents\n",
        "# You can stick strings together with + symbols to make it one big string\n",
        "prompt = pdf_text + \"\\n\\n\" + \"What is the name of the author of this document\"\n",
        "\n",
        "# Use query_llm (imported from hyperspecific.py) to get the LLM's answer!\n",
        "llm_answer = query_llm(prompt)\n",
        "\n",
        "print(\"Ollama LLM says:\\n\")\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "dPQnEbbXJnWj",
        "outputId": "15f6a08e-5f81-4d61-a0a2-8b6ab3e30c33"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3562592370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import PyPDF2 for PDF reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Using the PyPDF library, we can write a function that will read a Pdf and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fonzzy1/LLM-Workshop/blob/main/workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs for Communications Methods\n",
        "\n",
        "## Before you proceed\n",
        "---\n",
        "\n",
        "Since by default the runtime type of Colab instance is CPU based, in order to use LLM models make sure to change your runtime type to T4 GPU (or better if you're a paid Colab user). This can be done by going to **Runtime > Change runtime type**.\n",
        "\n",
        "While running your script be mindful of the resources you're using. This can be tracked at **Runtime > View resources**.\n"
      ],
      "metadata": {
        "id": "zyGk-87qnbWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install ollama\n",
        "import subprocess\n",
        "proccess = subprocess.Popen(['ollama', 'serve'])\n",
        "\n"
      ],
      "metadata": {
        "id": "YlVK9iG4AD5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pulling Model\n",
        "---\n",
        "We are going to be using a few models today: lamma, quen, gemma and deepseek. All of these models are open soruces, and all but gemma are the same size. However, they will exhibit slight differences when en\n",
        "\n",
        "For other models check https://ollama.com/library"
      ],
      "metadata": {
        "id": "WcBLqZfyoHg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.1:8b\n",
        "!ollama pull qwen3:8b\n",
        "!ollama pull gemma3:4b\n",
        "!ollama pull deepseek-r1:8b\n"
      ],
      "metadata": {
        "id": "o2ghppmRDFny",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama PyPDF2"
      ],
      "metadata": {
        "id": "MbrT39oil6tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "def query_llm(prompt, model=\"llama3.2\"):\n",
        "    \"\"\"\n",
        "    Given a 'prompt' and the name of a model,\n",
        "    return the LLM's text response (uses ollama SDK).\n",
        "    Because the model has a default, we don't need to be explicit in which model\n",
        "    to use if you don't want to.\n",
        "    \"\"\"\n",
        "    # Send the request to Ollama and get the response dictionary (a kind of\n",
        "    # \"named list\").\n",
        "    response = ollama.chat(\n",
        "        model=model, messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    # Return ONLY the LLM's textual answer from the response.\n",
        "    return response.message.content\n",
        "\n",
        "\n",
        "# --- Using (calling) a function ---\n",
        "# To use a function, write its name and provide any needed input (\"arguments\")\n",
        "# in parentheses.\n",
        "\n",
        "# Let's set up a question prompt for the LLM:\n",
        "prompt = (\n",
        "    \"What is the state of the art method for computational framining analysis?\"\n",
        ")\n",
        "\n",
        "# Now, call our function with this prompt to get the LLM's answer:\n",
        "llm_answer = query_llm(prompt)\n",
        "\n",
        "# Finally, print out the LLM's answer for everyone to see!\n",
        "print(\"Ollama LLM says:\\n\")\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "id": "9quBP56zDvpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import PyPDF2\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def read_pdf_text_from_url(pdf_url):\n",
        "    text = \"\"\n",
        "\n",
        "    response = requests.get(pdf_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    pdf_file = BytesIO(response.content)\n",
        "    reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    for page in reader.pages:\n",
        "        extracted = page.extract_text()\n",
        "        if extracted:\n",
        "            text += extracted\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# ---- Main workflow ----\n",
        "\n",
        "pdf_url = \"https://raw.githubusercontent.com/Fonzzy1/LLM-Workshop/cf5ea6b93655cebe909c01915ba67b55be7c6f31/example.pdf\"\n",
        "\n",
        "pdf_text = read_pdf_text_from_url(pdf_url)\n",
        "\n",
        "prompt = pdf_text + \"\\n\\n\" + \"What is the current state of the art for framing analysis\"\n",
        "\n",
        "llm_answer = query_llm(prompt)\n",
        "\n",
        "print(\"Ollama LLM says:\\n\")\n",
        "print(llm_answer)\n"
      ],
      "metadata": {
        "id": "dPQnEbbXJnWj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}